{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 05:16:50.597256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-26 05:16:50.597298: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19750e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c45901",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68652783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"rumus + data skripsi angka polos jarak dan visibilitas 2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_with_ratio(ratio):\n",
    "    # Shuffle the data\n",
    "    shuffled_data = data\n",
    "\n",
    "    # Get unique classes from the \"Peruntukan\" column\n",
    "    classes = shuffled_data['Peruntukan'].unique()\n",
    "\n",
    "    # Initialize empty dataframes for train and test data\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each class\n",
    "    for cls in classes:\n",
    "        cls_data = shuffled_data[shuffled_data['Peruntukan'] == cls]\n",
    "\n",
    "        # Split the class data into train and test\n",
    "        cls_train, cls_test = train_test_split(cls_data, test_size=ratio, random_state = 78,shuffle = False)\n",
    "\n",
    "        # Append the train and test data to the respective dataframes\n",
    "        train_data = train_data.append(cls_train)\n",
    "        test_data = test_data.append(cls_test)\n",
    "        \n",
    "    training = train_data\n",
    "    test = test_data\n",
    "\n",
    "    training = training[[\"Peruntukan\",\"Jarak_pusat_kota2\",\"Visibilitas\",\"Bangunan\",\"Luas\"]]\n",
    "    test = test[[\"Peruntukan\",\"Jarak_pusat_kota2\",\"Visibilitas\",\"Bangunan\",\"Luas\"]]\n",
    "\n",
    "    print(training)\n",
    "    print(test)\n",
    "    \n",
    "    return training, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10686067",
   "metadata": {},
   "source": [
    "# Data visualisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357eabc",
   "metadata": {},
   "source": [
    "# Change categorical to number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_categorical_to_number(data):\n",
    "    \n",
    "    data = data\n",
    "    \n",
    " \n",
    "    ####\n",
    "    ####\n",
    "\n",
    "    condition = [  data.Visibilitas == \"Strategis\",\n",
    "                 data.Visibilitas == \"Sedang\",\n",
    "                  data.Visibilitas == \"Kurang\",\n",
    "    ]\n",
    "\n",
    "    value = [3,2,1]\n",
    "\n",
    "    data.Visibilitas = np.select(condition,value)\n",
    "    #####\n",
    "    #####\n",
    "\n",
    "    condition = [  data.Bangunan == \"Bagus\",\n",
    "                 data.Bangunan == \"Sedang\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    value = [2,1]\n",
    "\n",
    "    data.Bangunan = np.select(condition,value,0)\n",
    "\n",
    "\n",
    "    condition = [ \n",
    "                data.Peruntukan == \"Pasar\",\n",
    "                 data.Peruntukan == \"Kantor\",\n",
    "                 data.Peruntukan == \"Ruko\",\n",
    "                 data.Peruntukan == \"Taman\",\n",
    "                 data.Peruntukan == \"Perumahan\",\n",
    "                 data.Peruntukan == \"Sawah\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    value = [5,4,3,2,1,0]\n",
    "\n",
    "    data.Peruntukan = np.select(condition,value,0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38057e5c",
   "metadata": {},
   "source": [
    "# Dataframe to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a60108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=False, batch_size=1):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('Peruntukan')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e24a5",
   "metadata": {},
   "source": [
    "# Feature Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8498ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_features(train_ds):\n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "\n",
    "    # Numerical features.\n",
    "    for header in [\"Jarak_pusat_kota2\",\"Visibilitas\",\"Bangunan\",\"Luas\"]:\n",
    "      numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "      normalization_layer = get_normalization_layer(header, train_ds)\n",
    "      encoded_numeric_col = normalization_layer(numeric_col)\n",
    "      all_inputs.append(numeric_col)\n",
    "      encoded_features.append(encoded_numeric_col)\n",
    "    return encoded_features, all_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a73e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b25bac",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(encoded_features,all_inputs):\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(6)(x)\n",
    "\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=\"accuracy\",\n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8b046",
   "metadata": {},
   "source": [
    "# History build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history(model,callbacks,train_ds,val_ds):\n",
    "    \n",
    "\n",
    "    history = model.fit(train_ds, epochs=150, validation_data=val_ds, callbacks=callbacks)\n",
    "\n",
    "    # Get the training and validation metrics from the history\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    max_test_accuracy_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "\n",
    "    # Get the corresponding training accuracy\n",
    "    training_accuracy = history.history['accuracy'][max_test_accuracy_index]\n",
    "\n",
    "    print('Best validation accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Training accuracy at the best test accuracy:', training_accuracy)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8db84",
   "metadata": {},
   "source": [
    "# Evaluation the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f132b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(ratio,callbacks):\n",
    "    \n",
    "    #split data\n",
    "    training, test_2 = get_data_with_ratio(ratio)\n",
    "    \n",
    "    #change data to number\n",
    "    training = change_categorical_to_number(training)\n",
    "    test = change_categorical_to_number(test_2)\n",
    "    \n",
    "    #df to ds\n",
    "    \n",
    "    train_ds = df_to_dataset(training)\n",
    "    val_ds = df_to_dataset(test)\n",
    "    \n",
    "    #encoded features\n",
    "    \n",
    "    encoded,inputs = encoded_features(train_ds)\n",
    "\n",
    "    \n",
    "    #set the model\n",
    "    model_engine = model(encoded,inputs)\n",
    "    \n",
    "    #run and evaluate the model\n",
    "    model_engine2 = history(model_engine,callbacks,train_ds,val_ds)\n",
    "    \n",
    "    #evaluate confusion matrix\n",
    "    confussion_matrix(model_engine2,val_ds,test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd320d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confussion_matrix(best_model, val_ds, test_data):\n",
    "    from sklearn.metrics import confusion_matrix   \n",
    "    model = best_model\n",
    "    \n",
    "    y_pred = model.model.predict(val_ds)\n",
    "\n",
    "    # Ubah output prediksi menjadi label kelas\n",
    "    label_kelas = [0, 1, 2, 3, 4, 5]\n",
    "    y_pred_label = [label_kelas[np.argmax(prediksi)] for prediksi in y_pred]\n",
    "\n",
    "    # Test labels\n",
    "    test_labels = test_data.Peruntukan\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(test_labels, y_pred_label)\n",
    "    \n",
    "    val_accuracy = model.history['val_accuracy']\n",
    "    train_accuracy = model.history['accuracy']\n",
    "\n",
    "    # Find the index where validation accuracy is highest\n",
    "    max_val_accuracy_index = np.argmax(val_accuracy)\n",
    "\n",
    "    # Find the index of the closest training accuracy to the maximum validation accuracy\n",
    "    closest_train_accuracy_index = np.argmin([abs(i - max_val_accuracy_index) for i in range(len(train_accuracy))])\n",
    "\n",
    "    print('Best validation accuracy:', val_accuracy[max_val_accuracy_index])\n",
    "    print('Closest training accuracy to the maximum validation accuracy:', train_accuracy[closest_train_accuracy_index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    \n",
    "    confusion_matrix = cm\n",
    "    \n",
    "    true_positives = np.diagonal(confusion_matrix)\n",
    "\n",
    "    # Step 3: Calculate the accuracy percentage for each class\n",
    "    class_data_totals = np.sum(confusion_matrix, axis=0)\n",
    "    class_accuracies = true_positives / class_data_totals * 100\n",
    "\n",
    "    # Step 4: Calculate the overall accuracy percentage\n",
    "    total_instances = np.sum(confusion_matrix)\n",
    "    overall_accuracy = np.sum(true_positives) / total_instances * 100\n",
    "\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    true_positives = np.diagonal(confusion_matrix)\n",
    "    false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "    false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "\n",
    "    # Step 3: Calculate precision, recall, and F1 score for each label\n",
    "    accuracy = true_positives / np.sum(confusion_matrix, axis=1)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Step 4: Calculate the total accuracy\n",
    "    total_accuracy = np.sum(true_positives) / np.sum(confusion_matrix) * 100\n",
    "    total_precision = np.mean(precision)\n",
    "    total_recall = np.mean(recall)\n",
    "    total_f1 = np.mean(f1_score)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Total Accuracy:\", total_accuracy)\n",
    "    print(\"Total Precision:\", total_precision)\n",
    "    print(\"Total Recall:\", total_recall)\n",
    "    \n",
    "    print(\"TP for each label\", true_positives)\n",
    "    print(\"Total data for each label\",  np.sum(confusion_matrix, axis=1))\n",
    "    print(\"FP for each label\", np.sum(confusion_matrix, axis=0) - true_positives)\n",
    "    print(\"FN for each label\", np.sum(confusion_matrix, axis=1) - true_positives)\n",
    "    # Print the results\n",
    "    print(\"Accuracy for each label:\", class_accuracies)\n",
    "    print(\"Precision for each label:\", precision)\n",
    "    print(\"Recall for each label:\", recall)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfcb77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fecd5a3",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de9978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd072430",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_1 = ModelCheckpoint(\"50.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_1 = 0.5\n",
    "\n",
    "callback_2= ModelCheckpoint(\"40.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_2 = 0.4\n",
    "\n",
    "callback_3= ModelCheckpoint(\"30.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_3 = 0.3\n",
    "\n",
    "callback_4= ModelCheckpoint(\"20.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_4 = 0.2\n",
    "\n",
    "callback_5= ModelCheckpoint(\"10.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_5 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_1, callback_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_2, callback_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c22601",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_3, callback_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_4, callback_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f889158",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_5, callback_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd9982",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f976c7",
   "metadata": {},
   "source": [
    "# Print the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57767fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test_2 = get_data_with_ratio(0.5)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "\n",
    "    \n",
    "    #change data to number\n",
    "training = change_categorical_to_number(training)\n",
    "test = change_categorical_to_number(test_2)\n",
    "\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = df_to_dataset(training)\n",
    "val_ds = df_to_dataset(test)\n",
    "    \n",
    "    #encoded features\n",
    "dataset = train_ds  # Your dataset here\n",
    "\n",
    "# Create an iterator for the dataset\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Get the next element from the iterator\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            # Fetch and print the next element from the dataset\n",
    "            element = sess.run(next_element)\n",
    "            print(element)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # This exception will be raised when all elements have been iterated through\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_ds  # Your dataset here\n",
    "\n",
    "# Create an iterator for the dataset\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Get the next element from the iterator\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            # Fetch and print the next element from the dataset\n",
    "            element = sess.run(next_element)\n",
    "            print(element)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # This exception will be raised when all elements have been iterated through\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea178a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded,inputs = encoded_features(train_ds)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695dc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d353d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
