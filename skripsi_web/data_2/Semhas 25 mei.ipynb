{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f09496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 04:53:58.697225: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-26 04:53:58.697269: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19750e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c45901",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68652783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"rumus + data skripsi angka polos jarak dan visibilitas 2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d56b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_with_ratio(ratio):\n",
    "    # Shuffle the data\n",
    "    shuffled_data = shuffle(data, random_state=42)\n",
    "\n",
    "    # Get unique classes from the \"Peruntukan\" column\n",
    "    classes = shuffled_data['Peruntukan'].unique()\n",
    "\n",
    "    # Initialize empty dataframes for train and test data\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each class\n",
    "    for cls in classes:\n",
    "        cls_data = shuffled_data[shuffled_data['Peruntukan'] == cls]\n",
    "\n",
    "        # Split the class data into train and test\n",
    "        cls_train, cls_test = train_test_split(cls_data, test_size=ratio, random_state=42,shuffle = False)\n",
    "\n",
    "        # Append the train and test data to the respective dataframes\n",
    "        train_data = train_data.append(cls_train)\n",
    "        test_data = test_data.append(cls_test)\n",
    "        \n",
    "        training = train_data\n",
    "        test = test_data\n",
    "\n",
    "        training = training[[\"Peruntukan\",\"Jarak pusat kota2\",\"Visibilitas\",\"Bangunan\",\"Luas\"]]\n",
    "        test = test[[\"Peruntukan\",\"Jarak pusat kota2\",\"Visibilitas\",\"Bangunan\",\"Luas\"]]\n",
    "\n",
    "\n",
    "    \n",
    "    return training, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850c28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10686067",
   "metadata": {},
   "source": [
    "# Data visualisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357eabc",
   "metadata": {},
   "source": [
    "# Change categorical to number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47f89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_categorical_to_number(data):\n",
    "    \n",
    "    data = data\n",
    "    \n",
    " \n",
    "    ####\n",
    "    ####\n",
    "\n",
    "    condition = [  data.Visibilitas == \"Strategis\",\n",
    "                 data.Visibilitas == \"Sedang\",\n",
    "                  data.Visibilitas == \"Kurang\",\n",
    "    ]\n",
    "\n",
    "    value = [3,2,1]\n",
    "\n",
    "    data.Visibilitas = np.select(condition,value)\n",
    "    #####\n",
    "    #####\n",
    "\n",
    "    condition = [  data.Bangunan == \"Bagus\",\n",
    "                 data.Bangunan == \"Sedang\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    value = [2,1]\n",
    "\n",
    "    data.Bangunan = np.select(condition,value,0)\n",
    "\n",
    "\n",
    "    condition = [ \n",
    "                data.Peruntukan == \"Pasar\",\n",
    "                 data.Peruntukan == \"Kantor\",\n",
    "                 data.Peruntukan == \"Ruko\",\n",
    "                 data.Peruntukan == \"Taman\",\n",
    "                 data.Peruntukan == \"Perumahan\",\n",
    "                 data.Peruntukan == \"Sawah\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    value = [5,4,3,2,1,0]\n",
    "\n",
    "    data.Peruntukan = np.select(condition,value,0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38057e5c",
   "metadata": {},
   "source": [
    "# Dataframe to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a60108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=False, batch_size=4):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('Peruntukan')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e24a5",
   "metadata": {},
   "source": [
    "# Feature Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9a634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8498ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_features(train_ds):\n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "\n",
    "    # Numerical features.\n",
    "    for header in [\"Jarak pusat kota2\",\"Visibilitas\",\"Bangunan\",\"Luas\"]:\n",
    "      numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "      normalization_layer = get_normalization_layer(header, train_ds)\n",
    "      encoded_numeric_col = normalization_layer(numeric_col)\n",
    "      all_inputs.append(numeric_col)\n",
    "      encoded_features.append(encoded_numeric_col)\n",
    "    return encoded_features, all_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a73e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b25bac",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b2e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(encoded_features,all_inputs):\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(6)(x)\n",
    "\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=\"accuracy\",\n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8b046",
   "metadata": {},
   "source": [
    "# History build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d84779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history(model,callbacks,train_ds,val_ds):\n",
    "    \n",
    "\n",
    "    history = model.fit(train_ds, epochs=150, validation_data=val_ds, callbacks=callbacks)\n",
    "\n",
    "    # Get the training and validation metrics from the history\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    max_test_accuracy_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "\n",
    "    # Get the corresponding training accuracy\n",
    "    training_accuracy = history.history['accuracy'][max_test_accuracy_index]\n",
    "\n",
    "    print('Best validation accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Training accuracy at the best test accuracy:', training_accuracy)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8db84",
   "metadata": {},
   "source": [
    "# Evaluation the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f132b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(ratio,callbacks):\n",
    "    \n",
    "    #split data\n",
    "    training, test_2 = get_data_with_ratio(ratio)\n",
    "    \n",
    "    #change data to number\n",
    "    training = change_categorical_to_number(training)\n",
    "    test = change_categorical_to_number(test_2)\n",
    "    \n",
    "    #df to ds\n",
    "    \n",
    "    train_ds = df_to_dataset(training)\n",
    "    val_ds = df_to_dataset(test)\n",
    "    \n",
    "    #encoded features\n",
    "    \n",
    "    encoded,inputs = encoded_features(train_ds)\n",
    "\n",
    "    \n",
    "    #set the model\n",
    "    model_engine = model(encoded,inputs)\n",
    "    \n",
    "    #run and evaluate the model\n",
    "    model_engine2 = history(model_engine,callbacks,train_ds,val_ds)\n",
    "    \n",
    "    #evaluate confusion matrix\n",
    "    confussion_matrix(model_engine2,val_ds,test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd320d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confussion_matrix(best_model, val_ds,test_data):\n",
    "    from sklearn.metrics import confusion_matrix   \n",
    "    model =  best_model\n",
    "    \n",
    "    y_pred = model.model.predict(val_ds)\n",
    "\n",
    "    # Ubah output prediksi menjadi label kelas\n",
    "    label_kelas = [0, 1, 2, 3, 4, 5]\n",
    "    y_pred_label = [label_kelas[np.argmax(prediksi)] for prediksi in y_pred]\n",
    "\n",
    "    #test label\n",
    "    test_labels = test_data.Peruntukan\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(test_labels, y_pred_label)\n",
    "    \n",
    "    max_test_accuracy_index = model.history['val_accuracy'].index(max(model.history['val_accuracy']))\n",
    "\n",
    "    # Get the corresponding training accuracy\n",
    "    training_accuracy = model.history['accuracy'][max_test_accuracy_index]\n",
    "\n",
    "    print('Best validation accuracy:', max(model.history['val_accuracy']))\n",
    "    print('Training accuracy at the best test accuracy:', training_accuracy)\n",
    "\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    \n",
    "    confusion_matrix = cm\n",
    "    \n",
    "    true_positives = np.diagonal(confusion_matrix)\n",
    "\n",
    "    # Step 3: Calculate the accuracy percentage for each class\n",
    "    class_data_totals = np.sum(confusion_matrix, axis=0)\n",
    "    class_accuracies = true_positives / class_data_totals * 100\n",
    "\n",
    "    # Step 4: Calculate the overall accuracy percentage\n",
    "    total_instances = np.sum(confusion_matrix)\n",
    "    overall_accuracy = np.sum(true_positives) / total_instances * 100\n",
    "\n",
    "    \n",
    "    for label, accuracy in zip(label_kelas, class_accuracies):\n",
    "        print(\"Accuracy for\", label, \":\", accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    true_positives = np.diagonal(confusion_matrix)\n",
    "    false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "    false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "\n",
    "    # Step 3: Calculate precision, recall, and F1 score for each label\n",
    "    accuracy = true_positives / np.sum(confusion_matrix, axis=1)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Step 4: Calculate the total accuracy\n",
    "    total_accuracy = np.sum(true_positives) / np.sum(confusion_matrix) * 100\n",
    "    total_precision = np.mean(precision)\n",
    "    total_recall = np.mean(recall)\n",
    "    total_f1 = np.mean(f1_score)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Total Precision:\", total_precision)\n",
    "    print(\"Total Recall:\", total_recall)\n",
    "    print(\"Total F1 Score:\", total_f1)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Accuracy for each label:\", class_accuracies)\n",
    "    print(\"Precision for each label:\", precision)\n",
    "    print(\"Recall for each label:\", recall)\n",
    "    print(\"F1 Score for each label:\", f1_score)\n",
    "    print(\"Total Accuracy:\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfcb77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fecd5a3",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd072430",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_1 = ModelCheckpoint(\"50.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_1 = 0.5\n",
    "\n",
    "callback_2= ModelCheckpoint(\"40.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_2 = 0.4\n",
    "\n",
    "callback_3= ModelCheckpoint(\"30.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_3 = 0.3\n",
    "\n",
    "callback_4= ModelCheckpoint(\"20.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_4 = 0.2\n",
    "\n",
    "callback_5= ModelCheckpoint(\"10.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "ratio_5 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f349cb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260903/2428549447.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(cls_train)\n",
      "/tmp/ipykernel_260903/2428549447.py:21: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_data = test_data.append(cls_test)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Jarak pusat kota2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratio_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(ratio, callbacks)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_model\u001b[39m(ratio,callbacks):\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#split data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     training, test_2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_with_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#change data to number\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     training \u001b[38;5;241m=\u001b[39m change_categorical_to_number(training)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_data_with_ratio\u001b[0;34m(ratio)\u001b[0m\n\u001b[1;32m     23\u001b[0m     training \u001b[38;5;241m=\u001b[39m train_data\n\u001b[1;32m     24\u001b[0m     test \u001b[38;5;241m=\u001b[39m test_data\n\u001b[0;32m---> 26\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPeruntukan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJarak pusat kota2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVisibilitas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBangunan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLuas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m     test \u001b[38;5;241m=\u001b[39m test[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeruntukan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJarak pusat kota2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisibilitas\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBangunan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLuas\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training, test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Jarak pusat kota2'] not in index\""
     ]
    }
   ],
   "source": [
    "run_model(ratio_1, callback_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_2, callback_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c22601",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_3, callback_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_4, callback_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f889158",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ratio_5, callback_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd9982",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f976c7",
   "metadata": {},
   "source": [
    "# N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de9978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57767fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
