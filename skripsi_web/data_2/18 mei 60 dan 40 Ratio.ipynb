{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa995850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c45901",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9981c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"rumus + data skripsi angka polos jarak dan visibilitas 2.csv\")\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_data = shuffle(data, random_state=42)\n",
    "\n",
    "# Calculate the split index based on the desired 50:50 ratio\n",
    "split_index = int(len(shuffled_data) * 0.5)\n",
    "\n",
    "# Split the data into two halves\n",
    "data_1 = shuffled_data[:split_index]\n",
    "data_2 = shuffled_data[split_index:]\n",
    "\n",
    "# Verify the lengths of the split data\n",
    "print(f\"Data 1 length: {len(data_1)}\")\n",
    "print(f\"Data 2 length: {len(data_2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cbf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_data = shuffle(data, random_state=42)\n",
    "\n",
    "# Get unique classes from the \"Peruntukan\" column\n",
    "classes = shuffled_data['Peruntukan'].unique()\n",
    "\n",
    "# Initialize empty dataframes for train and test data\n",
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over each class\n",
    "for cls in classes:\n",
    "    cls_data = shuffled_data[shuffled_data['Peruntukan'] == cls]\n",
    "    \n",
    "    # Split the class data into train and test\n",
    "    cls_train, cls_test = train_test_split(cls_data, test_size=0.4, random_state=42)\n",
    "    \n",
    "    # Append the train and test data to the respective dataframes\n",
    "    train_data = train_data.append(cls_train)\n",
    "    test_data = test_data.append(cls_test)\n",
    "\n",
    "# Verify the lengths of the split data\n",
    "print(f\"Train data length: {len(train_data)}\")\n",
    "print(f\"Test data length: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ef675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfcd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train_data\n",
    "test = test_data\n",
    "\n",
    "training = training[[\"Peruntukan\",\"Pusat_kota\",\"Visibilitas\",\"Bangunan\",\"Luas\"]]\n",
    "test = test[[\"Peruntukan\",\"Pusat_kota\",\"Visibilitas\",\"Bangunan\",\"Luas\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10686067",
   "metadata": {},
   "source": [
    "# Data visualisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357eabc",
   "metadata": {},
   "source": [
    "# Change categorical to number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = [  training.Pusat_kota == \"Pusat\",\n",
    "             training.Pusat_kota == \"Sedang\",\n",
    "              training.Pusat_kota == \"Pinggir\",\n",
    "]\n",
    "\n",
    "value = [3,2,1]\n",
    "\n",
    "training.Pusat_kota = np.select(condition,value)\n",
    "####\n",
    "####\n",
    "\n",
    "condition = [  training.Visibilitas == \"Strategis\",\n",
    "             training.Visibilitas == \"Sedang\",\n",
    "              training.Visibilitas == \"Kurang\",\n",
    "]\n",
    "\n",
    "value = [3,2,1]\n",
    "\n",
    "training.Visibilitas = np.select(condition,value)\n",
    "#####\n",
    "#####\n",
    "\n",
    "condition = [  training.Bangunan == \"Bagus\",\n",
    "             training.Bangunan == \"Sedang\",\n",
    "             \n",
    "]\n",
    "\n",
    "value = [2,1]\n",
    "\n",
    "training.Bangunan = np.select(condition,value,0)\n",
    "\n",
    "\n",
    "condition = [ \n",
    "            training.Peruntukan == \"Pasar\",\n",
    "             training.Peruntukan == \"Kantor\",\n",
    "             training.Peruntukan == \"Ruko\",\n",
    "             training.Peruntukan == \"Taman\",\n",
    "             training.Peruntukan == \"Perumahan\",\n",
    "             training.Peruntukan == \"Sawah\",\n",
    "             \n",
    "]\n",
    "\n",
    "value = [5,4,3,2,1,0]\n",
    "\n",
    "training.Peruntukan = np.select(condition,value,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = [  test.Pusat_kota == \"Pusat\",\n",
    "             test.Pusat_kota == \"Sedang\",\n",
    "              test.Pusat_kota == \"Pinggir\",\n",
    "]\n",
    "\n",
    "value = [3,2,1]\n",
    "\n",
    "test.Pusat_kota = np.select(condition,value)\n",
    "####\n",
    "####\n",
    "\n",
    "condition = [  test.Visibilitas == \"Strategis\",\n",
    "             test.Visibilitas == \"Sedang\",\n",
    "              test.Visibilitas == \"Kurang\",\n",
    "]\n",
    "\n",
    "value = [3,2,1]\n",
    "\n",
    "test.Visibilitas = np.select(condition,value)\n",
    "#####\n",
    "#####\n",
    "\n",
    "condition = [  test.Bangunan == \"Bagus\",\n",
    "             test.Bangunan == \"Sedang\",\n",
    "             \n",
    "]\n",
    "\n",
    "value = [2,1]\n",
    "\n",
    "test.Bangunan = np.select(condition,value,0)\n",
    "\n",
    "\n",
    "condition = [ \n",
    "            test.Peruntukan == \"Pasar\",\n",
    "             test.Peruntukan == \"Kantor\",\n",
    "             test.Peruntukan == \"Ruko\",\n",
    "             test.Peruntukan == \"Taman\",\n",
    "             test.Peruntukan == \"Perumahan\",\n",
    "             test.Peruntukan == \"Sawah\",\n",
    "             \n",
    "]\n",
    "\n",
    "value = [5,4,3,2,1,0]\n",
    "\n",
    "test.Peruntukan = np.select(condition,value,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38057e5c",
   "metadata": {},
   "source": [
    "# Dataframe to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a60108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=False, batch_size=4):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('Peruntukan')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(training)\n",
    "val_ds = df_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e24a5",
   "metadata": {},
   "source": [
    "# Feature Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8498ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numerical features.\n",
    "for header in [\"Pusat_kota\",\"Visibilitas\",\"Bangunan\",\"Luas\"]:\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc13fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "  if output_bias is not None:\n",
    "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "  model = keras.Sequential([\n",
    "      keras.layers.Dense(\n",
    "          16, activation='relu',\n",
    "          input_shape=(train_features.shape[-1],)),\n",
    "      keras.layers.Dropout(0.5),\n",
    "      keras.layers.Dense(1, activation='sigmoid',\n",
    "                         bias_initializer=output_bias),\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b25bac",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(6)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=\"accuracy\",\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `rankdir='LR'` to make the graph horizontal.\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12ede0",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "history = model.fit(train_ds, epochs=100, validation_data=val_ds, callbacks=[checkpoint_callback])\n",
    "\n",
    "# Get the training and validation metrics from the history\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(train_accuracy, label='Training Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa084c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_accuracy_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "\n",
    "# Get the corresponding training accuracy\n",
    "training_accuracy = history.history['accuracy'][max_test_accuracy_index]\n",
    "\n",
    "print('Best validation accuracy:', max(history.history['val_accuracy']))\n",
    "print('Training accuracy at the best test accuracy:', training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055323d",
   "metadata": {},
   "source": [
    "# Confusion Matriks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "best_model = tf.keras.models.load_model('./best_model.h5')\n",
    "model = best_model\n",
    "\n",
    "y_pred = model.predict(val_ds)\n",
    "\n",
    "# Ubah output prediksi menjadi label kelas\n",
    "label_kelas = ['Sawah', 'Perumahan', 'Taman', 'Ruko', 'Kantor', 'Pasar']\n",
    "y_pred_label = [label_kelas[np.argmax(prediksi)] for prediksi in y_pred]\n",
    "\n",
    "#test label\n",
    "test_labels = test_data.Peruntukan\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(test_labels, y_pred_label)\n",
    "\n",
    "print('Best validation accuracy:', max(history.history['val_accuracy']))\n",
    "print('Training accuracy at the best test accuracy:', training_accuracy)\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c14f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = cm\n",
    "\n",
    "true_positives = np.diagonal(confusion_matrix)\n",
    "\n",
    "# Step 3: Calculate the accuracy percentage for each class\n",
    "class_totals = np.sum(confusion_matrix, axis=1)\n",
    "class_accuracies = true_positives / class_totals * 100\n",
    "\n",
    "# Step 4: Calculate the overall accuracy percentage\n",
    "total_instances = np.sum(confusion_matrix)\n",
    "overall_accuracy = np.sum(true_positives) / total_instances * 100\n",
    "\n",
    "# Step 5: Calculate the total data instances for each class\n",
    "class_data_totals = np.sum(confusion_matrix, axis=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"True Positives for each class:\", true_positives)\n",
    "print(\"Total Data Instances for each class:\", class_data_totals)\n",
    "print(\"Overall Accuracy Percentage:\", overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3265e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = np.diagonal(confusion_matrix)\n",
    "\n",
    "# Step 3: Calculate the accuracy percentage for each class\n",
    "class_data_totals = np.sum(confusion_matrix, axis=0)\n",
    "class_accuracies = true_positives / class_data_totals * 100\n",
    "\n",
    "# Step 4: Calculate the overall accuracy percentage\n",
    "total_instances = np.sum(confusion_matrix)\n",
    "overall_accuracy = np.sum(true_positives) / total_instances * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy for each label:\", class_accuracies)\n",
    "print(\"True Positives for each class:\", true_positives)\n",
    "print(\"Total Data Instances for each class:\", class_data_totals)\n",
    "print(\"Overall Accuracy Percentage:\", overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, accuracy in zip(label_kelas, class_accuracies):\n",
    "    print(\"Accuracy for\", label, \":\", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = np.diagonal(confusion_matrix)\n",
    "false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "\n",
    "# Step 3: Calculate precision, recall, and F1 score for each label\n",
    "accuracy = true_positives / np.sum(confusion_matrix, axis=1)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Step 4: Calculate the total accuracy\n",
    "total_accuracy = np.sum(true_positives) / np.sum(confusion_matrix) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy for each label:\", class_accuracies)\n",
    "print(\"Precision for each label:\", precision)\n",
    "print(\"Recall for each label:\", recall)\n",
    "print(\"F1 Score for each label:\", f1_score)\n",
    "print(\"Total Accuracy:\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8264db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = np.diagonal(confusion_matrix)\n",
    "false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "\n",
    "# Step 3: Calculate precision, recall, and F1 score for each label\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Step 4: Calculate the total accuracy\n",
    "total_accuracy = np.sum(true_positives) / np.sum(confusion_matrix) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"True Positives for each label:\", true_positives)\n",
    "print(\"total classes\",class_data_totals)\n",
    "print(\"tp+fp pembagi precision\",true_positives + false_positives )\n",
    "print(\"tp+fn pembagi recall\",true_positives + false_negatives )\n",
    "print(\"f1 p kali r\", (precision * recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36804b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_accuracy_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "\n",
    "# Get the corresponding training accuracy\n",
    "training_accuracy = history.history['accuracy'][max_test_accuracy_index]\n",
    "\n",
    "print('Best validation accuracy:', max(history.history['val_accuracy']))\n",
    "print('Training accuracy at the best test accuracy:', training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26225db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4580e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(val_ds)\n",
    "\n",
    "         #   test.Peruntukan == \"Pasar\",\n",
    "          #   test.Peruntukan == \"Kantor\",\n",
    "           #  test.Peruntukan == \"Ruko\",\n",
    "            # test.Peruntukan == \"Taman\",\n",
    "             #test.Peruntukan == \"Perumahan\",\n",
    "             #test.Peruntukan == \"Sawah\",\n",
    "            \n",
    "         #   value = [5,4,3,2,1,0]\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a18b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah output prediksi menjadi label kelas\n",
    "label_kelas = ['Sawah', 'Perumahan', 'Taman', 'Ruko', 'Kantor', 'Pasar']\n",
    "y_pred_label = [label_kelas[np.argmax(prediksi)] for prediksi in y_pred]\n",
    "\n",
    "# Tampilkan hasil prediksi\n",
    "print(y_pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ed3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in y_pred_label:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72294ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(data, label_column):\n",
    "    # Get the unique labels\n",
    "    labels = data[label_column].unique()\n",
    "\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "\n",
    "    for label in labels:\n",
    "        # Get data points for the current label\n",
    "        label_data = data[data[label_column] == label].sample(frac=1)  # Shuffle the data points\n",
    "\n",
    "        if len(label_data) >= 2:\n",
    "            test_set.append(label_data.iloc[0])  # Add one data point to the test set\n",
    "            train_set.extend(label_data.iloc[1:])  # Add remaining data points to the train set\n",
    "        elif len(label_data) == 1:\n",
    "            test_set.append(label_data.iloc[0])  # Add the only data point to the test set\n",
    "\n",
    "    random.shuffle(train_set)  # Shuffle the train set\n",
    "    random.shuffle(test_set)  # Shuffle the test set\n",
    "\n",
    "    train_set = pd.DataFrame(train_set)\n",
    "    test_set = pd.DataFrame(test_set)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "# Example usage\n",
    "data = your_data_df  # Replace with your actual DataFrame\n",
    "label_column = 'label'  # Replace with the column name for the labels\n",
    "\n",
    "train_set, test_set = split_data(data, label_column)\n",
    "\n",
    "print(\"Train set:\")\n",
    "print(train_set)\n",
    "print()\n",
    "print(\"Test set:\")\n",
    "print(test_set)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
